Project: Optimizing an ML Pipeline in Azure
1.	Overview

This is a project in the Udacity (Azure machine learning engineering using Azure) Nanodegree. In this project, we used two approach including 1) Hyper-drive (using logistic regression algorithms), 2) also we used Azure Auto ML to go through all of different algorithm. My steps in this project are including 1) Create a Tabular Dataset from this bank marketing dataset and evaluate it with using logistic regression model, 2) Next, using Hyper-Drive and choosing different parameters and to find the best hyper parameters for the logistic regression model, 3)Then loaed the bank dataset using “TabularDatasetFactory” and use AutoML to find the best model. AutoML go through all the models and choose the best, and 4) finally compare the model in two approaches and write a research report.

2.	Summary
This dataset contains different variables about individual customers that applying for the loans in the bank. Based on all different independent variables such as previous loan, housing, marital, education, employed and etc we will try to predict the new customer will subscribe or not in the bank. Based on our result, the best performing model was voting Ensemble with 91.92% accuracy in AzureMl. However, other model also have close comparable accuracy including XGBoost Classifier and StackEnsemble.


Scikit-learn Pipeline
Here, we explained the pipeline architecture, including data, hyperparameter tuning, and classification algorithm. The steps are including: 
1)	Download the dataset
2)	Cleaning the dataset:
A)	Removing missing values from the dataset.
B)	One-hot encoding the variables including  job titles, contact, and education 
C)	Encoding months of the year.
D)	Encoding the target variable.
3)	Split the dataset to train and test set by ration of 33%
4)	Logistic regression as a classification method was used  and the parameters available within code show the regularization strength and maximum number of iterations.

Azure's Hyperdrive was used for tuning the hyperparameter with the following main elements that including:


1)	Parameter sampling:I used random parameter sampling because compare to the grid search and bayesian parameter sampling it takes less time to go through all the values of parameters and random parameter sampling enables a much more broad search over the parameter space and with it's capabilities to make use of concurrency would outperform bayesian sampling for large jobs. This is the codes I used in this step
ps = RandomParameterSampling(
{
'--C' : choice(0,10),
      '--max_iter': choice(50,100,200,300)})

2)	Early stopping policy: I used the BanditPolicy stopping policy because is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade.
“policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)”


AutoML
Here, I described the model and hyperparameters generated by AutoML.
The autoML pipeline is very similar to Hyper-drive with several distinguished differences. Steps are including
1)	The data are retrieved from the provided URL.
2)	The data are cleaned 
3)	Choose the y variable as the dependent variable
4)	The joined dataset is used as input in the autoML configuration and the autoML run is processed locally.
The best model selected by autoML was a voting ensemble (~91.92% accurate). 
These are different parameters that I used for the AutoMl:
automl_config = AutoMLConfig(
    compute_target = compute_target,
    experiment_timeout_minutes=15,
    task='classification',
    primary_metric='accuracy',
    training_data=ds,
    label_column_name='y',
    enable_onnx_compatible_models=True,
    n_cross_validations=4)
experiment_timeout_minutes=15)


Pipeline comparison
The accuracy of the two models was relatively similar with the hyperdive model (89.8%) accuracy and the autoML model (91.92%). This small different in the accuracy can be because of a variations in the cross-validation process. Also, AutoML test number of different algorithms and add a several preprocessing step prior to model training.Also, architecture of the model can be different in two approaches two
The votingEnsemble classifier with 91.92% accuracy achieved by AutoML that show high possibility of use of this approach in future.


Future work
This project show how useful is the using of automated approach in machine learning in training our model and with less time we can train several model in the same time. Furthermore, running AutoML for much longer would likely find better models in this case. Furthermore, exploring hyperdrive with a broader variety of classification models would also be informative. Also, in the future I will try to improve the n_cross_validations because normally higher cross validation lead to higher accuracy model. However, a high number also raises computation time thus costs so there must be a balance between the two factors. In addition, increasing the experiment time can make an improvement to this project. 

